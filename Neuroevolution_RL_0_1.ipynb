{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Neuroevolution-RL-0.1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvin-hansen/Reinforcement-Learning/blob/master/Neuroevolution_RL_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "50jgCj9ulL3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Deep Neuroevolution for Reinforcement Learning \n",
        "\n",
        "Basded on \n",
        "https://towardsdatascience.com/reinforcement-learning-without-gradients-evolving-agents-using-genetic-algorithms-8685817d84f\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "|    \t|               \t|\n",
        "|---------\t|-------------------------\t|\n",
        "| Author  \t| Marvin Hansen           \t|\n",
        "| Contact \t| marvin.hansen@gmail.com \t|\n",
        "| Version \t| 0.1                     \t|\n",
        "| Updated \t| April 30, 2019          \t|\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "---\n",
        "\n",
        "@TODO \n",
        "\n",
        "\n",
        "## Changelog\n",
        "\n",
        "---\n",
        "* V-0.1 [Apr/25] initial commit \n",
        "\n",
        "\n",
        "## Compatibility\n",
        "---\n",
        "\n",
        "Lib's\n",
        "\n",
        "\n",
        "\n",
        "GPU Acceleration \n",
        "* GPU: NVDIA Tesla T4 \n",
        "* Cuda V10.0.130\n"
      ]
    },
    {
      "metadata": {
        "id": "nKN1T6mEl6T8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installations "
      ]
    },
    {
      "metadata": {
        "id": "29sXo6AHlKSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# None yet "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lv4zPweomtXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "3-8pMbycmuku",
        "colab_type": "code",
        "outputId": "0f6f0f3d-e58f-40c2-8c75-67117640cc50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import platform\n",
        "print(\"Done\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fbz6Ee7cm_ce",
        "colab_type": "code",
        "outputId": "625969de-3102-44c0-cfaa-6fed07298b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"* Python Version: \" + str(platform.python_version()))\n",
        "#print(\"* Pandas Version: \" + str(pd.__version__))\n",
        "print(\"* Numpy Version: \" + str(np.__version__))\n",
        "\n",
        "print(\"* PyTorch Version: \" + str(torch.__version__))\n",
        "print()\n",
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Python Version: 3.6.7\n",
            "* Numpy Version: 1.16.3\n",
            "* PyTorch Version: 1.0.1.post2\n",
            "\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IfvZRikwnKdF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GPU Accelerations "
      ]
    },
    {
      "metadata": {
        "id": "isfKYW-6nOOD",
        "colab_type": "code",
        "outputId": "bcce4c2a-ee02-4fa4-93bf-e105202d66ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.current_device()\n",
        "\n",
        "print(\"Cuda available: \" + str(torch.cuda.is_available()))\n",
        "print(\"Cuda enabled:\" + str(torch.backends.cudnn.enabled))\n",
        "\n",
        "#https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(\"GPU used: \" + torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available: True\n",
            "Cuda enabled:True\n",
            "Using device: cuda\n",
            "\n",
            "GPU used: Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wrk_3iIFpXvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "a_QBZTtXpZQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CartPoleAI(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc = nn.Sequential(\n",
        "                        nn.Linear(4,128, bias=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(128,2, bias=True),\n",
        "                        nn.Softmax(dim=1)\n",
        "                        ) #.cuda()\n",
        "\n",
        "                \n",
        "        def forward(self, inputs):\n",
        "            x = self.fc(inputs)\n",
        "            return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59wnTCo4pchC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    \n",
        "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
        "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
        "        \n",
        "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
        "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
        "        \n",
        "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            m.bias.data.fill_(0.00)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cYhXpxcipfwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def return_random_agents(num_agents):\n",
        "    \n",
        "    agents = []\n",
        "    for _ in range(num_agents):\n",
        "        \n",
        "        agent = CartPoleAI() #.to(device)\n",
        "        \n",
        "        for param in agent.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        init_weights(agent)\n",
        "        agents.append(agent)\n",
        "        \n",
        "        \n",
        "    return agents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEk4JwNwp0kG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mutate(agent):\n",
        "\n",
        "    child_agent = copy.deepcopy(agent)\n",
        "    \n",
        "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
        "            \n",
        "    for param in child_agent.parameters():\n",
        "    \n",
        "        if(len(param.shape)==4): #weights of Conv2D\n",
        "\n",
        "            for i0 in range(param.shape[0]):\n",
        "                for i1 in range(param.shape[1]):\n",
        "                    for i2 in range(param.shape[2]):\n",
        "                        for i3 in range(param.shape[3]):\n",
        "                            \n",
        "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
        "                                \n",
        "                                    \n",
        "\n",
        "        elif(len(param.shape)==2): #weights of linear layer\n",
        "            for i0 in range(param.shape[0]):\n",
        "                for i1 in range(param.shape[1]):\n",
        "                    \n",
        "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
        "                        \n",
        "\n",
        "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
        "            for i0 in range(param.shape[0]):\n",
        "                \n",
        "                param[i0]+=mutation_power * np.random.randn()\n",
        "\n",
        "    return child_agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADUCXlvdpkw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_agents(agents):\n",
        "    \n",
        "    reward_agents = []\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    \n",
        "    for agent in agents:\n",
        "        agent.eval()\n",
        "    \n",
        "        observation = env.reset()\n",
        "        \n",
        "        r=0\n",
        "        s=0\n",
        "        \n",
        "        for _ in range(250):\n",
        "            \n",
        "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
        "            output_probabilities = agent(inp).detach().numpy()[0]\n",
        "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
        "            new_observation, reward, done, info = env.step(action)\n",
        "            r=r+reward\n",
        "            \n",
        "            s=s+1\n",
        "            observation = new_observation\n",
        "\n",
        "            if(done):\n",
        "                break\n",
        "\n",
        "        reward_agents.append(r)        \n",
        "        #reward_agents.append(s)\n",
        "        \n",
        "    \n",
        "    return reward_agents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bUmvVvvl4BJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_agents_cuda(agents):\n",
        "    \n",
        "    reward_agents = []\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    \n",
        "    for agent in agents:\n",
        "        agent.eval()\n",
        "        agent.to(device)\n",
        "    \n",
        "        observation = env.reset()\n",
        "        \n",
        "        r=0\n",
        "        s=0\n",
        "        \n",
        "        for _ in range(250):\n",
        "            \n",
        "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1) #.to(device)\n",
        "            output_probabilities = agent(inp).detach().numpy()[0]\n",
        "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
        "            new_observation, reward, done, info = env.step(action)\n",
        "            r=r+reward\n",
        "            \n",
        "            s=s+1\n",
        "            observation = new_observation\n",
        "\n",
        "            if(done):\n",
        "                break\n",
        "\n",
        "        reward_agents.append(r)        \n",
        "        #reward_agents.append(s)\n",
        "        \n",
        "    \n",
        "    return reward_agents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBhIY3Qppnlu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "sdCL1fJnpohG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def return_average_score(agent, runs):\n",
        "    score = 0.\n",
        "    for i in range(runs):\n",
        "        score += run_agents([agent])[0]\n",
        "        #score += run_agents_cuda([agent])[0]\n",
        "\n",
        "    return score/runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKvAAvVopqkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_agents_n_times(agents, runs):\n",
        "    avg_score = []\n",
        "    for agent in agents:\n",
        "        avg_score.append(return_average_score(agent,runs))\n",
        "    return avg_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqhzwDBypucO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuDJeNrhqa_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def return_children(agents, sorted_parent_indexes, elite_index):\n",
        "    \n",
        "    children_agents = []\n",
        "    \n",
        "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
        "    for i in range(len(agents)-1):\n",
        "        \n",
        "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
        "        children_agents.append(mutate(agents[selected_agent_index]))\n",
        "\n",
        "    #now add one elite\n",
        "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
        "    children_agents.append(elite_child)\n",
        "    elite_index=len(children_agents)-1 #it is the last one\n",
        "    \n",
        "    return children_agents, elite_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rG46cDDAp9Gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=5):\n",
        "    \n",
        "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
        "    \n",
        "    if(elite_index is not None):\n",
        "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
        "        \n",
        "    top_score = None\n",
        "    top_elite_index = None\n",
        "    \n",
        "    for i in candidate_elite_index:\n",
        "        score = return_average_score(agents[i],runs=5)\n",
        "        print(\"Score for elite i \", i, \" is \", score)\n",
        "        \n",
        "        if(top_score is None):\n",
        "            top_score = score\n",
        "            top_elite_index = i\n",
        "        elif(score > top_score):\n",
        "            top_score = score\n",
        "            top_elite_index = i\n",
        "            \n",
        "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
        "    \n",
        "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
        "    return child_agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zrz2rz7XqB2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run Genetic algo to find best agent"
      ]
    },
    {
      "metadata": {
        "id": "lg9TMIu9qE7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2621
        },
        "outputId": "a0627b34-fe18-4b49-9273-1f270b412be8"
      },
      "cell_type": "code",
      "source": [
        "game_actions = 2 #2 actions possible: left or right\n",
        "\n",
        "#disable gradients as we will not use them\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "# initialize N number of agents\n",
        "num_agents = 500\n",
        "agents = return_random_agents(num_agents)\n",
        "\n",
        "# How many top agents to consider as parents\n",
        "top_limit = 20\n",
        "\n",
        "# run evolution until X generations\n",
        "generations = 10 # 55\n",
        "\n",
        "elite_index = None\n",
        "\n",
        "best_agent = None\n",
        "\n",
        "for generation in range(generations):\n",
        "\n",
        "    # return rewards of agents\n",
        "    rewards = run_agents_n_times(agents, 3) #return average of 3 runs\n",
        "\n",
        "    # sort by rewards\n",
        "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    \n",
        "    top_rewards = []\n",
        "    for best_parent in sorted_parent_indexes:\n",
        "        top_rewards.append(rewards[best_parent])\n",
        "    \n",
        "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
        "    #print(rewards)\n",
        "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
        "    print(\"Rewards for top: \",top_rewards)\n",
        "    \n",
        "    print()\n",
        "    print(\"Best agent ID: \" + str(sorted_parent_indexes[0]))\n",
        "    print()\n",
        "    \n",
        "    best_agent_id = sorted_parent_indexes[0]\n",
        "    \n",
        "    best_agent = agents[best_agent_id]\n",
        "    \n",
        "    # setup an empty list for containing children agents\n",
        "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
        "\n",
        "    # kill all agents, and replace them with their children\n",
        "    agents = children_agents"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generation  0  | Mean rewards:  21.39  | Mean of top 5:  47.2\n",
            "Top  20  scores [137 369 100 332 190 235 158   5 280 300  72 189 475  16 171 346 124 184\n",
            "  75 426]\n",
            "Rewards for top:  [54.0, 49.0, 45.666666666666664, 44.0, 43.333333333333336, 43.0, 42.666666666666664, 42.333333333333336, 42.333333333333336, 41.333333333333336, 40.666666666666664, 39.666666666666664, 39.333333333333336, 39.0, 39.0, 38.666666666666664, 37.666666666666664, 37.666666666666664, 37.0, 36.333333333333336]\n",
            "\n",
            "Best agent ID: 137\n",
            "\n",
            "Score for elite i  137  is  26.6\n",
            "Score for elite i  369  is  29.2\n",
            "Score for elite i  100  is  32.2\n",
            "Score for elite i  332  is  22.6\n",
            "Score for elite i  190  is  20.8\n",
            "Elite selected with index  100  and score 32.2\n",
            "\n",
            "\n",
            "Generation  1  | Mean rewards:  24.02  | Mean of top 5:  61.266666666666666\n",
            "Top  20  scores [182  92 240 211 169 388 113 453  66 135 352 414 495  90 401 218 317  86\n",
            "  75 481]\n",
            "Rewards for top:  [74.33333333333333, 74.0, 55.666666666666664, 52.0, 50.333333333333336, 48.0, 47.666666666666664, 47.666666666666664, 47.666666666666664, 47.0, 46.666666666666664, 45.666666666666664, 45.666666666666664, 45.666666666666664, 42.333333333333336, 42.333333333333336, 42.333333333333336, 42.333333333333336, 40.666666666666664, 40.333333333333336]\n",
            "\n",
            "Best agent ID: 182\n",
            "\n",
            "Score for elite i  182  is  29.6\n",
            "Score for elite i  92  is  26.8\n",
            "Score for elite i  240  is  28.6\n",
            "Score for elite i  211  is  26.6\n",
            "Score for elite i  169  is  28.4\n",
            "Score for elite i  499  is  27.0\n",
            "Elite selected with index  182  and score 29.6\n",
            "\n",
            "\n",
            "Generation  2  | Mean rewards:  25.581333333333333  | Mean of top 5:  58.2\n",
            "Top  20  scores [158 305 187 191 232 385 476 440 254  71 412 289 479 454 465 496 484 103\n",
            " 265 281]\n",
            "Rewards for top:  [61.0, 60.333333333333336, 58.333333333333336, 55.666666666666664, 55.666666666666664, 50.666666666666664, 50.333333333333336, 49.666666666666664, 48.666666666666664, 48.0, 47.666666666666664, 47.666666666666664, 47.333333333333336, 46.666666666666664, 46.333333333333336, 46.0, 45.333333333333336, 45.0, 44.333333333333336, 43.333333333333336]\n",
            "\n",
            "Best agent ID: 158\n",
            "\n",
            "Score for elite i  158  is  29.8\n",
            "Score for elite i  305  is  29.6\n",
            "Score for elite i  187  is  24.4\n",
            "Score for elite i  191  is  19.2\n",
            "Score for elite i  232  is  20.0\n",
            "Score for elite i  499  is  31.4\n",
            "Elite selected with index  499  and score 31.4\n",
            "\n",
            "\n",
            "Generation  3  | Mean rewards:  27.592  | Mean of top 5:  61.0\n",
            "Top  20  scores [364 436 401  93 277 463 357  31 132 229 207  13 267 439 346 474 397 419\n",
            " 358 345]\n",
            "Rewards for top:  [71.33333333333333, 60.0, 58.333333333333336, 58.333333333333336, 57.0, 55.666666666666664, 55.333333333333336, 55.333333333333336, 54.666666666666664, 54.333333333333336, 54.333333333333336, 53.333333333333336, 53.333333333333336, 52.0, 52.0, 51.333333333333336, 51.0, 51.0, 50.666666666666664, 49.666666666666664]\n",
            "\n",
            "Best agent ID: 364\n",
            "\n",
            "Score for elite i  364  is  34.0\n",
            "Score for elite i  436  is  21.4\n",
            "Score for elite i  401  is  19.0\n",
            "Score for elite i  93  is  37.6\n",
            "Score for elite i  277  is  25.8\n",
            "Score for elite i  499  is  34.6\n",
            "Elite selected with index  93  and score 37.6\n",
            "\n",
            "\n",
            "Generation  4  | Mean rewards:  27.91  | Mean of top 5:  59.46666666666666\n",
            "Top  20  scores [345 239 413 327 488  29 376 295 121 397 187 163  55  94 443 468 424 256\n",
            "  97 300]\n",
            "Rewards for top:  [69.33333333333333, 62.333333333333336, 56.666666666666664, 55.666666666666664, 53.333333333333336, 52.333333333333336, 51.666666666666664, 50.333333333333336, 50.333333333333336, 50.333333333333336, 50.0, 49.333333333333336, 48.666666666666664, 48.333333333333336, 47.666666666666664, 47.0, 47.0, 46.666666666666664, 46.666666666666664, 46.333333333333336]\n",
            "\n",
            "Best agent ID: 345\n",
            "\n",
            "Score for elite i  345  is  25.2\n",
            "Score for elite i  239  is  25.8\n",
            "Score for elite i  413  is  29.8\n",
            "Score for elite i  327  is  24.0\n",
            "Score for elite i  488  is  26.8\n",
            "Score for elite i  499  is  37.8\n",
            "Elite selected with index  499  and score 37.8\n",
            "\n",
            "\n",
            "Generation  5  | Mean rewards:  30.094666666666665  | Mean of top 5:  66.00000000000001\n",
            "Top  20  scores [366  21  74  32 432 181  27 123  81 441 338 302 428 309 179 414 275  60\n",
            " 252 146]\n",
            "Rewards for top:  [85.33333333333333, 65.66666666666667, 61.333333333333336, 60.0, 57.666666666666664, 57.0, 57.0, 55.666666666666664, 55.666666666666664, 55.666666666666664, 55.333333333333336, 55.0, 54.666666666666664, 54.333333333333336, 54.0, 53.666666666666664, 53.0, 52.666666666666664, 52.333333333333336, 52.0]\n",
            "\n",
            "Best agent ID: 366\n",
            "\n",
            "Score for elite i  366  is  21.0\n",
            "Score for elite i  21  is  23.6\n",
            "Score for elite i  74  is  29.2\n",
            "Score for elite i  32  is  25.6\n",
            "Score for elite i  432  is  34.2\n",
            "Score for elite i  499  is  25.0\n",
            "Elite selected with index  432  and score 34.2\n",
            "\n",
            "\n",
            "Generation  6  | Mean rewards:  31.926666666666666  | Mean of top 5:  70.2\n",
            "Top  20  scores [339 485 284  58 171 457  24  27 366 121 212  76 112 269 489 335 202 483\n",
            " 336 447]\n",
            "Rewards for top:  [78.66666666666667, 70.66666666666667, 68.66666666666667, 67.0, 66.0, 65.33333333333333, 63.333333333333336, 62.666666666666664, 62.333333333333336, 61.666666666666664, 61.666666666666664, 60.666666666666664, 59.333333333333336, 59.0, 58.333333333333336, 58.333333333333336, 58.0, 57.666666666666664, 57.333333333333336, 56.666666666666664]\n",
            "\n",
            "Best agent ID: 339\n",
            "\n",
            "Score for elite i  339  is  44.6\n",
            "Score for elite i  485  is  37.8\n",
            "Score for elite i  284  is  28.0\n",
            "Score for elite i  58  is  19.8\n",
            "Score for elite i  171  is  29.0\n",
            "Score for elite i  499  is  49.0\n",
            "Elite selected with index  499  and score 49.0\n",
            "\n",
            "\n",
            "Generation  7  | Mean rewards:  30.974  | Mean of top 5:  68.8\n",
            "Top  20  scores [ 98 373  94 136  28 247   3  38  19 237 358 466 355  58 285 187 449 207\n",
            " 366 394]\n",
            "Rewards for top:  [76.0, 68.66666666666667, 67.0, 66.33333333333333, 66.0, 63.333333333333336, 61.666666666666664, 61.666666666666664, 58.333333333333336, 56.333333333333336, 56.333333333333336, 56.333333333333336, 55.333333333333336, 54.666666666666664, 54.666666666666664, 54.333333333333336, 53.0, 52.333333333333336, 52.0, 52.0]\n",
            "\n",
            "Best agent ID: 98\n",
            "\n",
            "Score for elite i  98  is  26.8\n",
            "Score for elite i  373  is  29.4\n",
            "Score for elite i  94  is  37.6\n",
            "Score for elite i  136  is  45.2\n",
            "Score for elite i  28  is  28.6\n",
            "Score for elite i  499  is  27.4\n",
            "Elite selected with index  136  and score 45.2\n",
            "\n",
            "\n",
            "Generation  8  | Mean rewards:  32.64266666666666  | Mean of top 5:  73.33333333333334\n",
            "Top  20  scores [233 187 103 262 438 183  41 385 447 266 240  35 159  16 348 480 342 400\n",
            " 281 426]\n",
            "Rewards for top:  [76.66666666666667, 75.33333333333333, 72.66666666666667, 71.66666666666667, 70.33333333333333, 64.33333333333333, 64.33333333333333, 64.0, 64.0, 62.0, 61.666666666666664, 61.333333333333336, 60.333333333333336, 60.0, 59.333333333333336, 58.666666666666664, 58.666666666666664, 57.666666666666664, 56.333333333333336, 55.666666666666664]\n",
            "\n",
            "Best agent ID: 233\n",
            "\n",
            "Score for elite i  233  is  47.2\n",
            "Score for elite i  187  is  18.6\n",
            "Score for elite i  103  is  34.4\n",
            "Score for elite i  262  is  28.6\n",
            "Score for elite i  438  is  40.6\n",
            "Score for elite i  499  is  30.8\n",
            "Elite selected with index  233  and score 47.2\n",
            "\n",
            "\n",
            "Generation  9  | Mean rewards:  33.89333333333334  | Mean of top 5:  78.6\n",
            "Top  20  scores [  1 395 178  56 155 408 412 394 369 459 158 237 270  82 124  77 258 197\n",
            " 433 334]\n",
            "Rewards for top:  [88.33333333333333, 83.66666666666667, 78.66666666666667, 73.33333333333333, 69.0, 68.66666666666667, 68.66666666666667, 67.66666666666667, 67.66666666666667, 67.33333333333333, 65.66666666666667, 64.66666666666667, 64.33333333333333, 62.666666666666664, 62.333333333333336, 62.0, 61.666666666666664, 61.666666666666664, 61.0, 61.0]\n",
            "\n",
            "Best agent ID: 1\n",
            "\n",
            "Score for elite i  1  is  27.2\n",
            "Score for elite i  395  is  26.4\n",
            "Score for elite i  178  is  37.2\n",
            "Score for elite i  56  is  39.2\n",
            "Score for elite i  155  is  30.4\n",
            "Score for elite i  499  is  32.0\n",
            "Elite selected with index  56  and score 39.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-HTEZjL00gBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "play_agent(agents[56])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDlwWWEOqC3N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def play_agent(agent):\n",
        "    try: #try and exception block because, render hangs if an erorr occurs, we must do env.close to continue working    \n",
        "        env = gym.make(\"CartPole-v0\")\n",
        "        \n",
        "        env_record = Monitor(env, './video', force=True)\n",
        "        observation = env_record.reset()\n",
        "        last_observation = observation\n",
        "        r=0\n",
        "        for _ in range(250):\n",
        "            env_record.render()\n",
        "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
        "            output_probabilities = agent(inp).detach().numpy()[0]\n",
        "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
        "            new_observation, reward, done, info = env_record.step(action)\n",
        "            r=r+reward\n",
        "            observation = new_observation\n",
        "\n",
        "            if(done):\n",
        "                break\n",
        "\n",
        "        env_record.close()\n",
        "        print(\"Rewards: \",r)\n",
        "\n",
        "    except Exception as e:\n",
        "        env_record.close()\n",
        "        print(e.__doc__)\n",
        "        print(e.message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBN-QOPUz21T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id = 90\n",
        "play_agent(agents[id])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBsTqlpY0T8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "play_agent(agents[96])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}